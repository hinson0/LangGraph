#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LangGraph 3.5 å®Œæ•´å¯è¿è¡Œæµ‹è¯•ç”¨ä¾‹
æ¼”ç¤ºå†…å®¹ï¼šè‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹ + é”™è¯¯å¤„ç† + æ¨¡å‹é™çº§æœºåˆ¶
"""

# å¯¼å…¥å¿…è¦çš„åº“
import json
import os
from dotenv import load_dotenv
from langchain_core.tools import tool
from langchain_core.messages import AIMessage, ToolMessage, HumanMessage
from langgraph.graph import MessagesState, StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field
from typing import Literal
from langchain_core.messages.modifier import RemoveMessage


# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# 1. å®šä¹‰å·¥å…·å‚æ•°æ¨¡å‹
class HaikuRequest(BaseModel):
    """ç”Ÿæˆè¯—çš„è¯·æ±‚å‚æ•°æ¨¡å‹"""

    topic: list[str] = Field(
        max_length=3, min_length=1, description="è¯—ä¸»é¢˜åˆ—è¡¨ï¼Œæœ€å¤š3ä¸ª"
    )


# 2. å®šä¹‰è‡ªå®šä¹‰å·¥å…·
@tool
def master_haiku_generator_tool(request: HaikuRequest):
    """ç”Ÿæˆä¸€ä¸ªè¯—ï¼ŒåŸºäºå¤šä¸ªç»™å®šçš„ä¸»é¢˜"""
    # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é…ç½®
    model = ChatOpenAI(
        model="Qwen/Qwen2.5-7B-Instruct",
        temperature=0.3,
        base_url=os.environ.get("OPENAI_BASE_URL"),
    )
    chain = model | StrOutputParser()
    topics = ", ".join(request.topic)
    haiku = chain.invoke(
        f"write a haiku about {topics}. Make sure it follows the 5-7-5 syllable pattern. Respond in Chinese."
    )
    return haiku


# 3. é…ç½®æ¨¡å‹
# åŸºç¡€æ¨¡å‹ï¼ˆä½¿ç”¨å¯ç”¨çš„æ¨¡å‹ï¼‰
model = ChatOpenAI(
    model="Qwen/Qwen2.5-7B-Instruct",  # æ›¿æ¢ä¸ºå¯ç”¨çš„æ¨¡å‹
    temperature=0.7,  # æé«˜æ¸©åº¦ï¼Œå¢åŠ ç”Ÿæˆå†…å®¹çš„å¯èƒ½æ€§
    base_url=os.environ.get("OPENAI_BASE_URL"),
)
model_with_tools = model.bind_tools([master_haiku_generator_tool])

# é™çº§æ¨¡å‹ï¼ˆä½¿ç”¨åŒä¸€ä¸ªå¯ç”¨æ¨¡å‹ï¼Œæˆ–æ›¿æ¢ä¸ºå…¶ä»–å¯ç”¨æ¨¡å‹ï¼‰
better_model = ChatOpenAI(
    model="Qwen/Qwen2.5-7B-Instruct",  # ä¿æŒä¸€è‡´æˆ–ä½¿ç”¨å…¶ä»–å¯ç”¨æ¨¡å‹
    temperature=0.7,  # æé«˜æ¸©åº¦ï¼Œå¢åŠ ç”Ÿæˆå†…å®¹çš„å¯èƒ½æ€§
    base_url=os.environ.get("OPENAI_BASE_URL"),
)
better_model_with_tools = better_model.bind_tools([master_haiku_generator_tool])


# 4. å®šä¹‰å·¥ä½œæµèŠ‚ç‚¹
def call_model_node(state: MessagesState):
    """è°ƒç”¨åŸºç¡€æ¨¡å‹èŠ‚ç‚¹"""
    print("\nğŸ“¤ è°ƒç”¨åŸºç¡€æ¨¡å‹...")
    print(f"   è¾“å…¥æ¶ˆæ¯æ•°é‡: {len(state['messages'])}")
    print(f"   æœ€åä¸€æ¡æ¶ˆæ¯ç±»å‹: {type(state['messages'][-1]).__name__}")
    print(f"   æœ€åä¸€æ¡æ¶ˆæ¯å†…å®¹: {state['messages'][-1].content[:100]}...")

    response = model_with_tools.invoke(state["messages"])

    print(f"   æ¨¡å‹å“åº”ç±»å‹: {type(response).__name__}")
    print(
        f"   æ¨¡å‹å“åº”å†…å®¹: {response.content[:100]}..."
        if response.content
        else "   æ¨¡å‹å“åº”å†…å®¹: (ç©º)"
    )
    if hasattr(response, "tool_calls") and response.tool_calls:
        print(f"   å·¥å…·è°ƒç”¨æ•°é‡: {len(response.tool_calls)}")
        print(f"   å·¥å…·è°ƒç”¨åç§°: {response.tool_calls[0]['name']}")
        print(f"   å·¥å…·è°ƒç”¨å‚æ•°: {response.tool_calls[0]['args']}")

    return {"messages": [response]}


def should_continue_node(state: MessagesState):
    """å†³ç­–èŠ‚ç‚¹ï¼šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·"""
    last_message = state["messages"][-1]
    print(f"\nğŸ” æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·...")
    print(f"   æœ€åä¸€æ¡æ¶ˆæ¯ç±»å‹: {type(last_message).__name__}")

    if last_message.tool_calls:
        print(f"ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {last_message.tool_calls[0]['name']}")
        return "call_tool_node"

    print("âœ… å¯¹è¯å®Œæˆï¼Œè¿”å›æœ€ç»ˆç»“æœ")
    return END


def call_tool_node(state: MessagesState):
    """è°ƒç”¨å·¥å…·èŠ‚ç‚¹ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
    tools_by_name = {master_haiku_generator_tool.name: master_haiku_generator_tool}
    messages = state["messages"]
    last_message = messages[-1]
    output_messages = []

    print("\nâš™ï¸ æ‰§è¡Œå·¥å…·è°ƒç”¨...")
    print(f"   å·¥å…·è°ƒç”¨æ•°é‡: {len(last_message.tool_calls)}")

    for tool_call in last_message.tool_calls:
        try:
            print(f"\n   æ‰§è¡Œå·¥å…·: {tool_call['name']}")
            print(f"   å‚æ•°: {tool_call['args']}")

            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
            print(f"   å·¥å…·æ‰§è¡ŒæˆåŠŸ!")
            print(f"   å·¥å…·ç»“æœ: {tool_result[:100]}...")

            # æ·»åŠ æˆåŠŸçš„å·¥å…·æ¶ˆæ¯
            output_messages.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        except Exception as e:
            print(f"   å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}")

            # æ·»åŠ åŒ…å«é”™è¯¯ä¿¡æ¯çš„å·¥å…·æ¶ˆæ¯
            output_messages.append(
                ToolMessage(
                    content=str(e),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                    additional_kwargs={"error": str(e)},
                )
            )

    return {"messages": output_messages}


def should_fallback_node(
    state: MessagesState,
) -> Literal["call_model_node", "remove_failed_tool_call_attempt_node"]:
    """å†³ç­–èŠ‚ç‚¹ï¼šæ˜¯å¦éœ€è¦é™çº§åˆ°æ›´å¥½çš„æ¨¡å‹"""
    messages = state["messages"]
    failed_tool_messages = [
        msg
        for msg in messages
        if isinstance(msg, ToolMessage) and msg.additional_kwargs.get("error")
    ]

    print("\nğŸ”„ æ£€æŸ¥å·¥å…·è°ƒç”¨ç»“æœ...")
    print(f"   å¤±è´¥çš„å·¥å…·è°ƒç”¨æ•°é‡: {len(failed_tool_messages)}")

    if failed_tool_messages:
        print("ğŸ”„ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨å¤±è´¥ï¼Œå‡†å¤‡é™çº§åˆ°æ›´å¥½çš„æ¨¡å‹")
        return "remove_failed_tool_call_attempt_node"

    print("ğŸ”„ å·¥å…·è°ƒç”¨æˆåŠŸï¼Œå›åˆ°æ¨¡å‹ç”Ÿæˆæœ€ç»ˆå›å¤")
    return "call_model_node"


def remove_failed_tool_call_attempt_node(state: MessagesState):
    """ç§»é™¤å¤±è´¥çš„å·¥å…·è°ƒç”¨å°è¯•"""
    print("\nğŸ—‘ï¸ ç§»é™¤å¤±è´¥çš„å·¥å…·è°ƒç”¨å†å²...")

    last_ai_message_index = next(
        i
        for i, msg in reversed(list(enumerate(state["messages"])))
        if isinstance(msg, AIMessage)
    )
    messages_to_remove = state["messages"][last_ai_message_index:]

    print(f"   è¦ç§»é™¤çš„æ¶ˆæ¯æ•°é‡: {len(messages_to_remove)}")
    for msg in messages_to_remove:
        print(f"   ç§»é™¤æ¶ˆæ¯: {type(msg).__name__}")

    return {"messages": [RemoveMessage(id=m.id) for m in messages_to_remove]}


def call_fallback_model_node(state: MessagesState):
    """è°ƒç”¨é™çº§æ¨¡å‹èŠ‚ç‚¹"""
    print("\nğŸ“¤ è°ƒç”¨é™çº§æ¨¡å‹...")
    print(f"   è¾“å…¥æ¶ˆæ¯æ•°é‡: {len(state['messages'])}")

    response = better_model_with_tools.invoke(state["messages"])

    print(f"   æ¨¡å‹å“åº”ç±»å‹: {type(response).__name__}")
    print(
        f"   æ¨¡å‹å“åº”å†…å®¹: {response.content[:100]}..."
        if response.content
        else "   æ¨¡å‹å“åº”å†…å®¹: (ç©º)"
    )
    if hasattr(response, "tool_calls") and response.tool_calls:
        print(f"   å·¥å…·è°ƒç”¨æ•°é‡: {len(response.tool_calls)}")

    return {"messages": [response]}


# 5. æ„å»ºå·¥ä½œæµå›¾
def build_workflow():
    """æ„å»ºå®Œæ•´çš„å·¥ä½œæµå›¾"""
    print("ğŸ—ï¸ æ„å»ºå·¥ä½œæµå›¾...")

    builder = StateGraph(MessagesState)

    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("call_model_node", call_model_node)
    builder.add_node("call_tool_node", call_tool_node)
    builder.add_node(
        "remove_failed_tool_call_attempt_node", remove_failed_tool_call_attempt_node
    )
    builder.add_node("call_fallback_model_node", call_fallback_model_node)

    # æ·»åŠ è¾¹
    builder.set_entry_point("call_model_node")
    builder.add_conditional_edges("call_model_node", should_continue_node)
    builder.add_conditional_edges("call_tool_node", should_fallback_node)
    builder.add_edge("remove_failed_tool_call_attempt_node", "call_fallback_model_node")
    builder.add_edge("call_fallback_model_node", "call_tool_node")

    # ç¼–è¯‘å·¥ä½œæµ
    return builder.compile()


# 6. æµ‹è¯•å‡½æ•°
def test_haiku_generation():
    """æµ‹è¯•è¯—ç”ŸæˆåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•ï¼šç”Ÿæˆè¯—")
    print("=" * 60)

    # æ„å»ºå·¥ä½œæµ
    graph = build_workflow()

    # å®šä¹‰æµ‹è¯•è¾“å…¥ - æ›´æ˜ç¡®åœ°è¯·æ±‚è¯—
    test_input = {
        "messages": [HumanMessage(content="è¯·ç”Ÿæˆä¸€é¦–å…³äºæ˜¥å¤©ã€æ¨±èŠ±å’Œå¸Œæœ›çš„è¯—")]
    }

    try:
        # è¿è¡Œå·¥ä½œæµ
        print("\nğŸš€ å¼€å§‹è¿è¡Œå·¥ä½œæµ...")
        result = graph.invoke(test_input)

        # æ‰“å°ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ‰ å·¥ä½œæµè¿è¡Œå®Œæˆï¼")
        print("=" * 60)

        # æ‰“å°æ‰€æœ‰æ¶ˆæ¯
        print("\nğŸ“ æ‰€æœ‰æ¶ˆæ¯å†å²:")
        print("-" * 40)
        for i, msg in enumerate(result["messages"]):
            print(f"æ¶ˆæ¯ {i + 1}: {type(msg).__name__}")
            if hasattr(msg, "content") and msg.content:
                print(f"å†…å®¹: {msg.content}")
            if hasattr(msg, "tool_calls") and msg.tool_calls:
                print(f"å·¥å…·è°ƒç”¨: {msg.tool_calls}")
            print("-" * 40)

        # è¾“å‡ºæœ€ç»ˆå›å¤
        final_message = result["messages"][-1]
        print(f"\nğŸ’¬ æœ€ç»ˆå›å¤:\n{final_message.content}")

        # å¦‚æœæœ€ç»ˆå›å¤ä¸ºç©ºï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·ç»“æœ
        if not final_message.content:
            print("\nâš ï¸ æ³¨æ„ï¼šæœ€ç»ˆå›å¤ä¸ºç©ºï¼")
            print("æ£€æŸ¥å·¥å…·ç»“æœæ˜¯å¦æ­£ç¡®ç”Ÿæˆ...")
            # æŸ¥æ‰¾å·¥å…·æ¶ˆæ¯
            tool_messages = [
                msg for msg in result["messages"] if isinstance(msg, ToolMessage)
            ]
            if tool_messages:
                print(f"\næ‰¾åˆ° {len(tool_messages)} æ¡å·¥å…·æ¶ˆæ¯:")
                for msg in tool_messages:
                    print(f"å·¥å…·æ¶ˆæ¯å†…å®¹: {msg.content}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        import traceback

        traceback.print_exc()


# 7. ä¸»å‡½æ•°
if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_haiku_generation()
